{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"collapsed_sections":[],"default_view":{},"name":"DNN_hands_on_solution.ipynb","provenance":[{"file_id":"1i6MJxD7kd04W6ojetXVPsNUeA4qGg6Zq","timestamp":1525158878710}],"version":"0.3.2","views":{}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":570014,"sourceType":"datasetVersion","datasetId":275486}],"dockerImageVersionId":28449,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Constant Learning Rate\n\nUntil now, throughout the training process, the learning rate remained constant.\n\nWhen the path taken by gradient descent is approaching minimum, it might bypass the minimum just because the gradient is large.\n\nThis will take additional steps to return to the minimum.\n\nWhen learning rate is constant, the gradient descent tends to oscillate when it is about to converge.\n\n\nThe training can start with a large learning rate since randomized weights will be far from the optimal. At later stages, the learning rate can be decreased to allow more fine-grained weight updates.\n\n\n\n### Batch Normalization\n\nFrom the previous course Building Effective Deep Neural Network, you have seen how normalizing inputs helps to train network faster.\n\nThis concept can be extended to each layer of the network by normalizing the output of the previous layer before feeding to the next layer. This technique is known as batch normalization.\n\n### Covariant Shift\n\nThe covariant shift is one of the major problems solved by batch normalization.\n\nFor example, if you have trained a network to identify human faces using grayscale images and then if you test your model on colored images, the network might not perform well since there is a large difference in pixel values between the train and test data.\n\nThis problem is known as covariant shift where there is a shift in the data distribution, but the ground truth remains the same.\n\n\n### Internal Covariant Shift\n\nCovariant shifts can also happen between the layers of the network when data flows across the layers.\n\nIn the case of mini batch gradient descent, since each batch is made of a set of random samples the current batch might have a different distribution compared to the previous mini batch.\n\nThis change in distribution might reflect in the output of subsequent layers. Otherwise, when the parameters of previous layers get updated, it also changes the input distribution for the current layer.\n\nIn batch normalization, each layer makes sure that its input distribution remains the same by normalizing $!Z^{[l]}Z​[l]$ before performing activation.\n\n\nFor current mini-batch\n\nHow it Works?\n\nThe equations shown in the previous card do the following operations:\n\nCalculate the mean $(\\mu)(μ)$ of the minibatch.\n\nCalculate variance $(\\sigma^2)(σ2​ )$ of the minibatch.\n\nCalculate $Z^{norm}Z$ norm​  by subtracting the mean from Z and dividing by the standard deviation $(\\sigma)(σ)$. A small number, epsilon $(\\epsilon)(ϵ)$, is added to the denominator to prevent divide by zero. Now the distribution has zero mean and unit variance.\n\nCalculate $\\tilde{Z}​Z​~$ by multiplying $Z^{norm}Z$\n​norm  with a scale $(\\gamma)(γ)$ and adding a shift $(\\beta)(β)$ and use $\\tilde{Z}​Z~$ \n​​  in place of Z as the nonlinearity (e.g. ReLU’s) input. The two parameters $\\betaβ$ and $\\gammaγ$ are learned during the training process with parameters W and b.\n\n\n\nParameters vs Hyperparameters\n\nTill now, you have come across several parameters and hyperparameters.\n\nParameters are the one that is learned by the network by performing gradient descent.\n\nWeights W, bias b, scaling parameters $\\gammaγ$ and $\\betaβ$ (the one which you learned in batch normalization) are the parameters that you initialize randomly and leave it for the network to learn.\n\nHyperparameters, on the other hand, cannot be learned from the data but has to be tried on different values within some range till we get the model right.\n\nHyperparameters\n\nSome of the important hyperparameters you have learned so far are:\n\nlearning rate $\\alphaα$\n\nparameter for the gradient with momentum $\\betaβ$\n\nnumber of nodes in each layer\n\nnumber of layers\n\nmini-batch size\n\n$\\beta_1β​1$​​  ,$\\beta_2β​2$  and $\\epsilonϵ$ with respect to adam optimizer\n\nSelecting Hyperparameters\n\nWhen training the model, you have to try on various combinations of parameters and come up with one set of parameters on which the model performs its best.\n\n\nGrid Search\nIn a grid search, you will arrange the parameters in the form of a matrix and try out each combination to train your model.\n\nIn a real scenario, when trying out more than two hyperparameters and matrix can be multidimensional.\n\nThe main problem with this approach is that you will end up training the number of models having the same accuracy.\n\nThis works well when the number of hyperparameters is small.\n\nRandom Search\nIn this approach instead of iterating through each of the combinations, we randomly select a limited number of combination to train our model.\n\nThough you are not iterating over all possible combination the chances of selecting the best combination will be high.\n\nThis is helpful when you have a large number of parameters to tune.\n\n\nChoosing Appropriate Scale\n\nHyperparameters like a number of nodes, a number of layers can be searched on a linear scale since their range is very small.\n\nThe model's performance is sensitive for a small change in values $\\alphaα$ or $\\betaβ$ hence searching for them on linear sale would be a bad idea.\n\n\n\n\nGD with momentum: to avoid too many oscillations in the path taken by gradient descent.\n\nRmsProp: a technique to have a balanced step size - decreases the step size in case of a larger gradient and increases the step size for vanishing gradient.\n\nAdam optimizer: an algorithm to combine the elements of momentum and RMSProp.\n\nBatch normalization: to prevent covariant shift by normalizing inputs of activation.\n\nHyperparameter tuning: methods to search for optimal hyperparameters.\n\nLearning rate decay: how to have control over the learning rate to prevent large gradient steps at during convergence.","metadata":{}},{"cell_type":"markdown","source":"Welcome to the first handson!!!\n- In this handson you will be building an deep neural network network by integrating batch normalization\n- You will also be implementing minibatch gradient and L2 regularization to train you network\n- Follow the instruction provided for cell to write the code in each cell.\n- Run the below cell for to import necessary packages to read and visualize data","metadata":{"colab_type":"text","id":"nF_5oOZ9OEaG"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"yaDtyHKHVAKL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is provided as file named 'data.csv'.  \nUsing pandas read the csv file and assign the resulting dataframe to variable 'data'   \nfor example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** ","metadata":{"colab_type":"text","id":"ivY_tF8yVKex"}},{"cell_type":"code","source":"data = pd.read_csv('../input/data (1).csv')\ndata.head()","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"0BHA1dAKyzw8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Extract feature1 and feature2 values from dataframe 'df' and assign it to variable 'X'\n- Extract target variable 'traget' and assign it to variable 'y'.  \nHint:\n - Use .values to exract values from dataframe","metadata":{"colab_type":"text","id":"rqCtS1D3Wi44"}},{"cell_type":"code","source":"\nX = data.loc[:, data.columns != 'target'].values\ny = data['target'].values\n","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"IaRJ8HLLXpuK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n- The green spots corresponds to target value 0 and blue spots corresponds to target value 1","metadata":{}},{"cell_type":"code","source":"colors=['green','blue']\ncmap = matplotlib.colors.ListedColormap(colors)\n#Plot the figure\nplt.figure()\nplt.title('Non-linearly separable classes')\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, cmap=cmap,\n            s=25, edgecolor='k')\nplt.show()","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"OLSGMjm0VGkJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n%matplotlib inline\ncolor_wheel = {0: \"#0392cf\", \n               1: \"#7bc043\", \n            }\n\ncolors_mapped = data[\"target\"].map(lambda x: color_wheel.get(x))\n\naxes_matrix = scatter_matrix(data.loc[:, data.columns != 'target'], alpha = 0.2, figsize = (10, 10), color=colors_mapped )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n- Transpose X and assign it to variable 'X_data'\n- reshape y to have shape (1, number of samples) and assign to variable 'y_data'","metadata":{}},{"cell_type":"code","source":"X_data = X.T\ny_data = y.reshape(1, -1)\n\nassert X_data.shape == (2, 1000)\nassert y_data.shape == (1, 1000)","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"XchlAZPwHpMk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the network dimension to have **two** input features, **four** **hidden layers** with **20** nodes each, one output node at final layer. ","metadata":{}},{"cell_type":"code","source":"\nlayer_dims = [2, 20, 20, 20, 20, 1]\n","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"t_dEEaJykLSa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import tensorflow as tf","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n- Set the datatype of placeholders as **float32**\n- parameters - num_features\n- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)","metadata":{}},{"cell_type":"code","source":"def placeholders(num_features):\n    \n    A_0 = tf.placeholder( shape=([num_features, None]), dtype=tf.float32)\n    Y = tf.placeholder(shape=([1,None]), dtype=tf.float32)\n    \n    return A_0,Y","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"4kp4_9kS58CT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"define function named initialize_parameters_deep() to initialize weights and bias for each layer\n- Use tf.get_variable to initialise weights and bias, set datatype as **float32**\n- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n- Parameters - layer_dims\n- Returns - dictionary of weights and bias","metadata":{}},{"cell_type":"code","source":"def initialize_parameters_deep(layer_dims):\n    tf.set_random_seed(1)\n    L = len(layer_dims)\n    parameters = {}\n    for l in range(1,L):\n        \n        parameters['W' + str(l)] = tf.get_variable('W'+ str(l), shape=([layer_dims[l], layer_dims[l-1]]), dtype=tf.float32,\n                                                  initializer=tf.contrib.layers.xavier_initializer())\n        parameters['b' + str(l)] = tf.get_variable('b'+ str(l), shape=([layer_dims[l], 1]), dtype=tf.float32, initializer=tf.zeros_initializer())\n        \n    return parameters ","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"W3l_vyXVkrlw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define functon named linear_forward_prop() to define forward propagation for a given layer.\n- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n- returns: A(output from the current layer)\n- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid\n- After computing linear output Z implement batch normalization before feeding to activation function, **set traing = True and axis = 0**","metadata":{}},{"cell_type":"code","source":"def linear_forward_prop(A_prev,W,b, activation):\n    \n    Z = tf.add(tf.matmul( W, A_prev), b)                          \n    #call linear_fowrward prop\n    Z = tf.layers.batch_normalization(inputs=Z, axis=0, training=True, gamma_initializer=tf.ones_initializer(), \n                                      beta_initializer=tf.zeros_initializer())                             \n    #implement batch normalization on Z\n    \n    if activation == \"sigmoid\":\n        A = Z\n    elif activation == \"relu\":\n        A = tf.nn.relu(Z)\n    return A","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"SKbSUt5g4toV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define forward propagation for entire network as l_layer_forward()\n- Parameters: A_0(input data), parameters(dictionary of weights and bias)\n- returns: A(output from final layer)  ","metadata":{}},{"cell_type":"code","source":"def l_layer_forwardProp(A_0, parameters):\n    A = A_0\n    L = len(parameters)//2\n    for l in range(1,L):\n        A_prev = A\n    \n        A = linear_forward_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu' )                 \n        #call linear forward prop with relu activation\n    A = linear_forward_prop(A, parameters['W' +str(L)], parameters['b' + str(L)], activation='sigmoid')                      \n    #call linear forward prop with sigmoid activation\n    \n    return A","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"Cdks1w__k-Ei","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Define the cost function\n- parameters:\n  - Z_final: output fro final layer\n  - Y: actual output\n  - parameters: dictionary of weigths and bias\n  - regularization : boolean\n  - lambd: regularization parameter\n- First define the original cost using tensoflow's sigmoid_cross_entropy function\n- If **regularization == True** add regularization term to original cost function","metadata":{}},{"cell_type":"code","source":"def final_cost(Z_final, Y , parameters, regularization = False, lambd = 0):\n    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y)\n    if regularization:\n        reg_term = 0\n        L = len(parameters)//2\n        for l in range(1,L+1):\n            \n            reg_term += tf.nn.l2_loss(parameters['W'+str(l)])             #add L2 loss term\n            \n        cost = cost + (lambd/2) * reg_term\n    return tf.reduce_mean(cost)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the function to generate mini-batches.","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef random_samples_minibatch(X, Y, batch_size, seed = 1):\n    np.random.seed(seed)\n    \n    m = X.shape[1]                                         #number of samples\n    num_batches =int( m / batch_size)                                 #number of batches derived from batch_size\n    \n    indices = np.random.permutation(m)                                  # generate ramdom indicies\n    shuffle_X = X[:,indices]\n    shuffle_Y = Y[:,indices]\n    mini_batches = []\n    \n    #generate minibatch\n    for i in range(num_batches):\n        X_batch = shuffle_X[ :, i * batch_size:(i+1) * batch_size]\n        Y_batch = shuffle_Y[ :, i * batch_size:(i+1) * batch_size]\n        \n        assert X_batch.shape == (X.shape[0], batch_size)\n        assert Y_batch.shape == (Y.shape[0], batch_size)\n        \n        mini_batches.append((X_batch, Y_batch))\n    \n    #generate batch with remaining number of samples\n    if m % batch_size != 0:\n        X_batch = shuffle_X[ :, (num_batches * batch_size): ]\n        Y_batch = shuffle_Y[:, (num_batches * batch_size): ]\n        mini_batches.append((X_batch, Y_batch))\n    return mini_batches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the model to train the network using minibatch\n- parameters:\n  - X_train, Y_train: input and target data\n  - layer_dims: network configuration\n  - learning_rate\n  - num_iter: number of epoches\n  - mini_batch_size: number of samples to be considered in each minibatch\n- return: dictionary of trained parameters","metadata":{}},{"cell_type":"code","source":"def model_with_minibatch(X_train,Y_train, layer_dims, learning_rate,num_iter, mini_batch_size):\n    tf.reset_default_graph()\n    num_features, num_samples = X_train.shape\n    \n    A_0, Y =   placeholders(num_features)\n    #call placeholder function to initialize placeholders A_0 and Y\n    parameters = initialize_parameters_deep(layer_dims)\n    #Initialse Weights and bias using initialize_parameters\n    Z_final =  l_layer_forwardProp(A_0, parameters) \n    #call the function l_layer_forwardProp() to define the final output\n    \n    cost = final_cost(Z_final, Y , parameters, regularization = True)\n    #call the final_cost function with regularization set TRUE\n    \n    \n    #use adam optimization to train the network\n    train_net = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999).minimize(cost)\n    \n    seed = 1\n    num_minibatches = int(num_samples / mini_batch_size)\n    init = tf.global_variables_initializer()\n    costs = []\n    with tf.Session() as sess:\n        sess.run(init)\n        for epoch in range(num_iter):\n            epoch_cost = 0\n            \n            mini_batches =  random_samples_minibatch(X_train, Y_train, mini_batch_size, seed)\n            #call random_sample_minibatch to return minibatches\n            \n            seed = seed + 1\n            \n            #perform gradient descent for each mini-batch\n            for mini_batch in mini_batches:\n                \n                X_batch, Y_batch = mini_batch \n                #assign minibatch\n                \n                _,mini_batch_cost = sess.run([train_net, cost], feed_dict={A_0: X_batch, Y: Y_batch})\n                \n                epoch_cost += mini_batch_cost/num_minibatches\n            if epoch % 2 == 0:\n                costs.append(epoch_cost)\n            if epoch % 100 == 0:\n                print(epoch_cost)\n        with open(\"output.txt\", \"w+\") as file:\n            file.write(\"%f\" % epoch_cost)\n        plt.ylim(0 ,2, 0.0001)\n        plt.xlabel(\"epoches per 2\")\n        plt.ylabel(\"cost\")\n        plt.plot(costs)\n        plt.show()\n        params = sess.run(parameters)\n    return params","metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"_aBIc5RIRz-q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train the model using the above defined function\n- Use X_data and y_data as training input, learning rate = 0.001, numiteration = 1000  \n  minibatch size = 256\n- Return the trained parameters to variable parameters","metadata":{}},{"cell_type":"code","source":"\nparameters =  model_with_minibatch(X_data,y_data, layer_dims, learning_rate=0.001,num_iter=1000, mini_batch_size=256)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}